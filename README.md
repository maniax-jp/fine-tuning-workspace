# Sarashina 2.2 0.5B Fine-tuning

æ—¥æœ¬èªè¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒSarashina 2.2 0.5Bã€ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

## ğŸ“‹ æ¦‚è¦

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€Hugging Face Transformersã‚’ä½¿ç”¨ã—ã¦Sarashina 2.2 0.5B Instructãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚

- **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**: [sbintuitions/sarashina2.2-0.5B-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5B-instruct-v0.1)
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°**: 0.79B
- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: Hugging Face Transformers
- **ç²¾åº¦**: BFloat16

## ğŸ³ ä½¿ç”¨ã™ã‚‹Dockerã‚¤ãƒ¡ãƒ¼ã‚¸

```bash
nvcr.io/nvidia/pytorch:25.01-py3
```

## ğŸ“¦ å¿…è¦ãªç’°å¢ƒ

- **GPU**: NVIDIA GPUï¼ˆCUDAå¯¾å¿œï¼‰
- **VRAM**: æœ€ä½16GBä»¥ä¸Šæ¨å¥¨ï¼ˆ32GBæ¨å¥¨ï¼‰
- **Docker**: NVIDIA Container Toolkit ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
- **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: ç´„20GBä»¥ä¸Šã®ç©ºãå®¹é‡

## ğŸš€ èµ·å‹•æ–¹æ³•

### 1. Dockerã‚³ãƒ³ãƒ†ãƒŠã®èµ·å‹•

```bash
docker run --rm -it \
  --gpus all \
  --shm-size=16g \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  -v $(pwd):/workspace \
  -w /workspace \
  nvcr.io/nvidia/pytorch:25.01-py3
```

**ã‚ªãƒ—ã‚·ãƒ§ãƒ³èª¬æ˜:**
- `--gpus all`: å…¨ã¦ã®GPUã‚’ä½¿ç”¨
- `--shm-size=16g`: å…±æœ‰ãƒ¡ãƒ¢ãƒªã‚’16GBã«è¨­å®š
- `--ulimit memlock=-1`: ãƒ¡ãƒ¢ãƒªãƒ­ãƒƒã‚¯åˆ¶é™ã‚’è§£é™¤
- `-v ./fine-tuning-workspace:/workspace`: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚¦ãƒ³ãƒˆ
- `-w /workspace`: ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š

### 2. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ

#### æ–¹æ³•A: ãƒ•ãƒ«ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆå…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰

ã‚³ãƒ³ãƒ†ãƒŠå†…ã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œï¼š

```bash
bash run_finetune_transformers.sh
```

ã¾ãŸã¯ã€æ‰‹å‹•ã§å®Ÿè¡Œï¼š

```bash
# ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -q transformers datasets accelerate sentencepiece protobuf

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
python finetune_with_transformers.py
```

#### æ–¹æ³•B: LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ¨å¥¨ãƒ»ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ï¼‰

```bash
bash run_finetune_lora.sh
```

ã¾ãŸã¯ã€æ‰‹å‹•ã§å®Ÿè¡Œï¼š

```bash
# ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -q transformers datasets accelerate sentencepiece protobuf peft bitsandbytes

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
python finetune_with_lora.py
```

**LoRAã®åˆ©ç‚¹:**
- ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤§å¹…ã«å‰Šæ¸›ï¼ˆç´„1/10ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
- âš¡ å­¦ç¿’é€Ÿåº¦ãŒé«˜é€Ÿ
- ğŸ’° è¨ˆç®—ã‚³ã‚¹ãƒˆãŒä½ã„
- ğŸ”„ è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’åˆ‡ã‚Šæ›¿ãˆå¯èƒ½

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
workspace/
â”œâ”€â”€ README.md                          # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ .gitignore                         # Gité™¤å¤–è¨­å®š
â”œâ”€â”€ finetune_with_transformers.py      # ãƒ•ãƒ«ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
â”œâ”€â”€ finetune_with_lora.py              # LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆNEW!ï¼‰
â”œâ”€â”€ run_finetune_transformers.sh       # ãƒ•ãƒ«ç‰ˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ run_finetune_lora.sh               # LoRAç‰ˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆNEW!ï¼‰
â”œâ”€â”€ inference_lora.py                  # LoRAãƒ¢ãƒ‡ãƒ«æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆNEW!ï¼‰
â”œâ”€â”€ train.jsonl                        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆ13,513ã‚µãƒ³ãƒ—ãƒ«ï¼‰
â”œâ”€â”€ valid.jsonl                        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ï¼ˆ1,502ã‚µãƒ³ãƒ—ãƒ«ï¼‰
â””â”€â”€ nemo_experiments/                  # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆ.gitignoreã§é™¤å¤–ï¼‰
    â”œâ”€â”€ sarashina_finetune_hf/         # ãƒ•ãƒ«ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‡ºåŠ›
    â”‚   â”œâ”€â”€ checkpoint-13500/          # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆeval_loss: 1.962ï¼‰
    â”‚   â”œâ”€â”€ checkpoint-20200/
    â”‚   â””â”€â”€ checkpoint-20271/          # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
    â””â”€â”€ sarashina_lora_finetune/       # LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å‡ºåŠ›
```

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¯ä»¥ä¸‹ã®JSON Lineså½¢å¼ï¼š

```json
{"input": "æŒ‡ç¤ºæ–‡ã‚„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", "output": "æœŸå¾…ã•ã‚Œã‚‹å¿œç­”", "source": "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å"}
```

## âš™ï¸ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š

### ãƒ•ãƒ«ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

```python
- ã‚¨ãƒãƒƒã‚¯æ•°: 3
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 1
- å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—: 2ï¼ˆå®Ÿè³ªãƒãƒƒãƒã‚µã‚¤ã‚º: 2ï¼‰
- å­¦ç¿’ç‡: 1e-5
- æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·: 2048
- ç²¾åº¦: BFloat16
- è©•ä¾¡é–“éš”: 100ã‚¹ãƒ†ãƒƒãƒ—
- ä¿å­˜é–“éš”: 100ã‚¹ãƒ†ãƒƒãƒ—
```

### LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

```python
- ã‚¨ãƒãƒƒã‚¯æ•°: 3
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 2
- å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—: 4ï¼ˆå®Ÿè³ªãƒãƒƒãƒã‚µã‚¤ã‚º: 8ï¼‰
- å­¦ç¿’ç‡: 2e-4ï¼ˆLoRAã¯é«˜ã‚ã®å­¦ç¿’ç‡ãŒåŠ¹æœçš„ï¼‰
- LoRAãƒ©ãƒ³ã‚¯ (r): 8
- LoRA Alpha: 32
- LoRAãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ: 0.1
- å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
- å­¦ç¿’å¯èƒ½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: ç´„10%
- æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·: 2048
- ç²¾åº¦: BFloat16
```

## ğŸ“ˆ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµæœ

### ãƒ•ãƒ«ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

- **è¨“ç·´æå¤±**: 3.93 â†’ 0.85
- **æ¤œè¨¼æå¤±**: 2.33 â†’ 1.98
- **ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°**: 20,271
- **æ‰€è¦æ™‚é–“**: ç´„3æ™‚é–“20åˆ†
- **ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«**: checkpoint-13500ï¼ˆeval_loss: 1.962ï¼‰

### LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

- **è¨“ç·´æå¤±**: å¹³å‡ 2.09
- **ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°**: 5,070
- **æ‰€è¦æ™‚é–“**: ç´„2æ™‚é–“28åˆ†ï¼ˆ8,885ç§’ï¼‰
- **å‡¦ç†é€Ÿåº¦**:
  - 4.56 ã‚µãƒ³ãƒ—ãƒ«/ç§’
  - 0.57 ã‚¹ãƒ†ãƒƒãƒ—/ç§’
- **ã‚¨ãƒãƒƒã‚¯æ•°**: 3
- **å­¦ç¿’ãƒ‡ãƒ¼ã‚¿**: 13,513ã‚µãƒ³ãƒ—ãƒ« Ã— 3ã‚¨ãƒãƒƒã‚¯ = 40,539å›ã®å­¦ç¿’

## ğŸ”§ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### å­¦ç¿’ç‡ã®å¤‰æ›´

`finetune_with_transformers.py`ã®`TrainingArguments`å†…ã§è¨­å®šï¼š

```python
learning_rate=1e-5  # ãŠå¥½ã¿ã®å€¤ã«å¤‰æ›´
```

### ã‚¨ãƒãƒƒã‚¯æ•°ã®å¤‰æ›´

```python
num_train_epochs=3  # ãŠå¥½ã¿ã®å€¤ã«å¤‰æ›´
```

### ãƒãƒƒãƒã‚µã‚¤ã‚ºã®èª¿æ•´

```python
per_device_train_batch_size=1  # VRAMå®¹é‡ã«å¿œã˜ã¦èª¿æ•´
gradient_accumulation_steps=2   # å®Ÿè³ªçš„ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã™
```

## ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨æ–¹æ³•

### ãƒ•ãƒ«ãƒ»ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
model_path = "./nemo_experiments/sarashina_finetune_hf/checkpoint-13500"
model = AutoModelForCausalLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# æ¨è«–
inputs = tokenizer("ã“ã‚“ã«ã¡ã¯ã€", return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
print(tokenizer.decode(outputs[0]))
```

### LoRAãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
base_model_path = "sbintuitions/sarashina2.2-0.5B-instruct-v0.1"
tokenizer = AutoTokenizer.from_pretrained(base_model_path)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é©ç”¨
lora_path = "./sarashina_0.5B_lora"
model = PeftModel.from_pretrained(base_model, lora_path)

# æ¨è«–
inputs = tokenizer("ã“ã‚“ã«ã¡ã¯ã€", return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
print(tokenizer.decode(outputs[0]))
```

ã¾ãŸã¯ã€æ¨è«–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ï¼š

```bash
python inference_lora.py
```

## ğŸ“ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ä½¿ç”¨ã—ã¦ã„ã‚‹Sarashina 2.2ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«ã¤ã„ã¦ã¯ã€[å…ƒã®ãƒ¢ãƒ‡ãƒ«ãƒšãƒ¼ã‚¸](https://huggingface.co/sbintuitions/sarashina2.2-0.5B-instruct-v0.1)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ğŸ™ è¬è¾

- ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: [sbintuitions/sarashina2.2-0.5B-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5B-instruct-v0.1)
- ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯: Hugging Face Transformers
- Dockerç’°å¢ƒ: NVIDIA PyTorch Container

## ğŸ“ ã‚µãƒãƒ¼ãƒˆ

å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€Issueã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
