# Sarashina 2.2 0.5B Fine-tuning

æ—¥æœ¬èªè¨€èªãƒ¢ãƒ‡ãƒ«ã€ŒSarashina 2.2 0.5Bã€ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

## ğŸ“‹ æ¦‚è¦

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€Hugging Face Transformersã‚’ä½¿ç”¨ã—ã¦Sarashina 2.2 0.5B Instructãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚

- **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«**: [sbintuitions/sarashina2.2-0.5B-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5B-instruct-v0.1)
- **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°**: 0.79B
- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: Hugging Face Transformers
- **ç²¾åº¦**: BFloat16

## ğŸ³ ä½¿ç”¨ã™ã‚‹Dockerã‚¤ãƒ¡ãƒ¼ã‚¸

```bash
nvcr.io/nvidia/pytorch:25.01-py3
```

## ğŸ“¦ å¿…è¦ãªç’°å¢ƒ

- **GPU**: NVIDIA GPUï¼ˆCUDAå¯¾å¿œï¼‰
- **VRAM**: æœ€ä½16GBä»¥ä¸Šæ¨å¥¨ï¼ˆ32GBæ¨å¥¨ï¼‰
- **Docker**: NVIDIA Container Toolkit ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
- **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: ç´„20GBä»¥ä¸Šã®ç©ºãå®¹é‡

## ğŸš€ èµ·å‹•æ–¹æ³•

### 1. Dockerã‚³ãƒ³ãƒ†ãƒŠã®èµ·å‹•

```bash
docker run --rm -it \
  --gpus all \
  --shm-size=16g \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  -v ./fine-tuning-workspace:/workspace \
  -w /workspace \
  nvcr.io/nvidia/pytorch:25.01-py3
```

**ã‚ªãƒ—ã‚·ãƒ§ãƒ³èª¬æ˜:**
- `--gpus all`: å…¨ã¦ã®GPUã‚’ä½¿ç”¨
- `--shm-size=16g`: å…±æœ‰ãƒ¡ãƒ¢ãƒªã‚’16GBã«è¨­å®š
- `--ulimit memlock=-1`: ãƒ¡ãƒ¢ãƒªãƒ­ãƒƒã‚¯åˆ¶é™ã‚’è§£é™¤
- `-v ./fine-tuning-workspace:/workspace`: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚¦ãƒ³ãƒˆ
- `-w /workspace`: ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š

### 2. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ

ã‚³ãƒ³ãƒ†ãƒŠå†…ã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œï¼š

```bash
bash run_finetune_transformers.sh
```

ã¾ãŸã¯ã€æ‰‹å‹•ã§å®Ÿè¡Œï¼š

```bash
# ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -q transformers datasets accelerate sentencepiece protobuf

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
python finetune_with_transformers.py
```

## ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
workspace/
â”œâ”€â”€ README.md                          # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ .gitignore                         # Gité™¤å¤–è¨­å®š
â”œâ”€â”€ finetune_with_transformers.py      # ãƒ¡ã‚¤ãƒ³ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ run_finetune_transformers.sh       # å®Ÿè¡Œç”¨ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ train.jsonl                        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆ13,513ã‚µãƒ³ãƒ—ãƒ«ï¼‰
â”œâ”€â”€ valid.jsonl                        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ï¼ˆ1,502ã‚µãƒ³ãƒ—ãƒ«ï¼‰
â””â”€â”€ nemo_experiments/                  # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆ.gitignoreã§é™¤å¤–ï¼‰
    â””â”€â”€ sarashina_finetune_hf/
        â”œâ”€â”€ checkpoint-13500/          # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆeval_loss: 1.962ï¼‰
        â”œâ”€â”€ checkpoint-20200/
        â””â”€â”€ checkpoint-20271/          # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
```

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¯ä»¥ä¸‹ã®JSON Lineså½¢å¼ï¼š

```json
{"input": "æŒ‡ç¤ºæ–‡ã‚„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", "output": "æœŸå¾…ã•ã‚Œã‚‹å¿œç­”", "source": "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å"}
```

## âš™ï¸ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š

```python
- ã‚¨ãƒãƒƒã‚¯æ•°: 3
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 1
- å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—: 2ï¼ˆå®Ÿè³ªãƒãƒƒãƒã‚µã‚¤ã‚º: 2ï¼‰
- å­¦ç¿’ç‡: 1e-5
- æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³é•·: 2048
- ç²¾åº¦: BFloat16
- è©•ä¾¡é–“éš”: 100ã‚¹ãƒ†ãƒƒãƒ—
- ä¿å­˜é–“éš”: 100ã‚¹ãƒ†ãƒƒãƒ—
```

## ğŸ“ˆ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°çµæœ

- **è¨“ç·´æå¤±**: 3.93 â†’ 0.85
- **æ¤œè¨¼æå¤±**: 2.33 â†’ 1.98
- **ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°**: 20,271
- **æ‰€è¦æ™‚é–“**: ç´„3æ™‚é–“20åˆ†
- **ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«**: checkpoint-13500ï¼ˆeval_loss: 1.962ï¼‰

## ğŸ”§ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### å­¦ç¿’ç‡ã®å¤‰æ›´

`finetune_with_transformers.py`ã®`TrainingArguments`å†…ã§è¨­å®šï¼š

```python
learning_rate=1e-5  # ãŠå¥½ã¿ã®å€¤ã«å¤‰æ›´
```

### ã‚¨ãƒãƒƒã‚¯æ•°ã®å¤‰æ›´

```python
num_train_epochs=3  # ãŠå¥½ã¿ã®å€¤ã«å¤‰æ›´
```

### ãƒãƒƒãƒã‚µã‚¤ã‚ºã®èª¿æ•´

```python
per_device_train_batch_size=1  # VRAMå®¹é‡ã«å¿œã˜ã¦èª¿æ•´
gradient_accumulation_steps=2   # å®Ÿè³ªçš„ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã™
```

## ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨æ–¹æ³•

ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
model_path = "./nemo_experiments/sarashina_finetune_hf/checkpoint-13500"
model = AutoModelForCausalLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# æ¨è«–
inputs = tokenizer("ã“ã‚“ã«ã¡ã¯ã€", return_tensors="pt")
outputs = model.generate(**inputs, max_length=100)
print(tokenizer.decode(outputs[0]))
```

## ğŸ“ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ä½¿ç”¨ã—ã¦ã„ã‚‹Sarashina 2.2ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«ã¤ã„ã¦ã¯ã€[å…ƒã®ãƒ¢ãƒ‡ãƒ«ãƒšãƒ¼ã‚¸](https://huggingface.co/sbintuitions/sarashina2.2-0.5B-instruct-v0.1)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ğŸ™ è¬è¾

- ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«: [sbintuitions/sarashina2.2-0.5B-instruct-v0.1](https://huggingface.co/sbintuitions/sarashina2.2-0.5B-instruct-v0.1)
- ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯: Hugging Face Transformers
- Dockerç’°å¢ƒ: NVIDIA PyTorch Container

## ğŸ“ ã‚µãƒãƒ¼ãƒˆ

å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€Issueã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
